apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-alerting-rules
  namespace: monitoring
data:
  alerts.yml: |
    groups:
    # ============================================
    # NODE ALERTS - Infrastructure Health
    # ============================================
    - name: node-alerts
      rules:
      - alert: NodeDown
        expr: up{job="node-exporter"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Node {{ $labels.instance }} is down"
          description: "Node exporter on {{ $labels.instance }} has been unreachable for more than 2 minutes."

      - alert: NodeHighCPU
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is above 85% (current: {{ $value | printf \"%.1f\" }}%)"

      - alert: NodeHighMemory
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 85% (current: {{ $value | printf \"%.1f\" }}%)"

      - alert: NodeHighMemoryCritical
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 95
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 95% (current: {{ $value | printf \"%.1f\" }}%)"

      - alert: NodeDiskSpaceLow
        expr: (1 - (node_filesystem_avail_bytes{fstype=~"ext4|xfs"} / node_filesystem_size_bytes{fstype=~"ext4|xfs"})) * 100 > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Low disk space on {{ $labels.instance }} ({{ $labels.mountpoint }})"
          description: "Disk usage is above 80% (current: {{ $value | printf \"%.1f\" }}%)"

      - alert: NodeDiskSpaceCritical
        expr: (1 - (node_filesystem_avail_bytes{fstype=~"ext4|xfs"} / node_filesystem_size_bytes{fstype=~"ext4|xfs"})) * 100 > 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical disk space on {{ $labels.instance }} ({{ $labels.mountpoint }})"
          description: "Disk usage is above 90% (current: {{ $value | printf \"%.1f\" }}%)"

      - alert: NodeDiskWillFillIn24Hours
        expr: predict_linear(node_filesystem_avail_bytes{fstype=~"ext4|xfs"}[6h], 24*60*60) < 0
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Disk on {{ $labels.instance }} will fill within 24 hours"
          description: "Based on current trends, {{ $labels.mountpoint }} will run out of space within 24 hours."

      - alert: NodeHighLoadAverage
        expr: node_load15 / count without(cpu, mode) (node_cpu_seconds_total{mode="idle"}) > 2
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "High load average on {{ $labels.instance }}"
          description: "15-minute load average is more than 2x the number of CPUs."

      - alert: NodeNetworkErrors
        expr: rate(node_network_receive_errs_total[5m]) + rate(node_network_transmit_errs_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Network errors on {{ $labels.instance }}"
          description: "Network interface {{ $labels.device }} is experiencing errors."

    # ============================================
    # KUBERNETES CLUSTER ALERTS
    # ============================================
    - name: kubernetes-alerts
      rules:
      - alert: KubernetesNodeNotReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes node {{ $labels.node }} is not ready"
          description: "Node has been in NotReady state for more than 5 minutes."

      - alert: KubernetesNodeMemoryPressure
        expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Memory pressure on node {{ $labels.node }}"
          description: "Node is under memory pressure."

      - alert: KubernetesNodeDiskPressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Disk pressure on node {{ $labels.node }}"
          description: "Node is under disk pressure."

      - alert: KubernetesPodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) * 60 * 15 > 3
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
          description: "Pod has restarted more than 3 times in the last 15 minutes."

      - alert: KubernetesPodNotReady
        expr: sum by(namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown"}) > 0
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is not ready"
          description: "Pod has been in Pending/Unknown state for more than 15 minutes."

      - alert: KubernetesDeploymentReplicasMismatch
        expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replica mismatch"
          description: "Deployment has {{ $value }} unavailable replicas."

      - alert: KubernetesStatefulSetReplicasMismatch
        expr: kube_statefulset_replicas != kube_statefulset_status_replicas_ready
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} replica mismatch"
          description: "StatefulSet has unavailable replicas."

      - alert: KubernetesDaemonSetNotScheduled
        expr: kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} not fully scheduled"
          description: "Some DaemonSet pods are not scheduled."

      - alert: KubernetesJobFailed
        expr: kube_job_status_failed > 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Job {{ $labels.namespace }}/{{ $labels.job_name }} failed"
          description: "Kubernetes job has failed."

      - alert: KubernetesPVCPending
        expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending"
          description: "PVC has been pending for more than 5 minutes."

    # ============================================
    # CONTAINER RESOURCE ALERTS
    # ============================================
    - name: container-alerts
      rules:
      - alert: ContainerHighCPU
        expr: sum by(namespace, pod, container) (rate(container_cpu_usage_seconds_total{container!=""}[5m])) / sum by(namespace, pod, container) (kube_pod_container_resource_limits{resource="cpu"}) * 100 > 90
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }} high CPU"
          description: "Container CPU usage is above 90% of its limit."

      - alert: ContainerHighMemory
        expr: sum by(namespace, pod, container) (container_memory_working_set_bytes{container!=""}) / sum by(namespace, pod, container) (kube_pod_container_resource_limits{resource="memory"}) * 100 > 90
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }} high memory"
          description: "Container memory usage is above 90% of its limit."

      - alert: ContainerOOMKilled
        expr: kube_pod_container_status_last_terminated_reason{reason="OOMKilled"} == 1
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }} was OOM killed"
          description: "Container was terminated due to out of memory."

    # ============================================
    # CRITICAL SERVICE ALERTS
    # ============================================
    - name: critical-services
      rules:
      # Ingress NGINX
      - alert: IngressNginxDown
        expr: kube_deployment_status_replicas_available{namespace="ingress-nginx",deployment="ingress-nginx-controller"} < 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Ingress NGINX controller is down"
          description: "No healthy ingress-nginx pods detected."

      - alert: IngressNginxHighErrorRate
        expr: sum(rate(nginx_ingress_controller_requests{status=~"5.."}[5m])) / sum(rate(nginx_ingress_controller_requests[5m])) * 100 > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High 5xx error rate on Ingress NGINX"
          description: "More than 5% of requests are returning 5xx errors."

      # Cert-Manager
      - alert: CertManagerDown
        expr: absent(kube_deployment_status_replicas_available{namespace="cert-manager",deployment="cert-manager"} >= 1)
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Cert-Manager is down"
          description: "No healthy cert-manager pods detected."

      - alert: CertificateExpiringSoon
        expr: certmanager_certificate_expiration_timestamp_seconds - time() < 7 * 24 * 60 * 60
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "Certificate {{ $labels.namespace }}/{{ $labels.name }} expiring soon"
          description: "Certificate will expire in less than 7 days."

      - alert: CertificateExpiryCritical
        expr: certmanager_certificate_expiration_timestamp_seconds - time() < 24 * 60 * 60
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Certificate {{ $labels.namespace }}/{{ $labels.name }} expiring within 24 hours"
          description: "Certificate will expire in less than 24 hours!"

      # ArgoCD
      - alert: ArgoCDDown
        expr: absent(kube_deployment_status_replicas_available{namespace="argocd",deployment="argocd-server"} >= 1)
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "ArgoCD server is down"
          description: "No healthy argocd-server pods detected."

      - alert: ArgoCDAppOutOfSync
        expr: argocd_app_info{sync_status!="Synced"} == 1
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "ArgoCD app {{ $labels.name }} is out of sync"
          description: "Application has been out of sync for more than 30 minutes."

      - alert: ArgoCDAppUnhealthy
        expr: argocd_app_info{health_status!~"Healthy|Progressing"} == 1
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "ArgoCD app {{ $labels.name }} is unhealthy"
          description: "Application health status: {{ $labels.health_status }}"

      # Longhorn
      - alert: LonghornVolumeUnhealthy
        expr: longhorn_volume_robustness != 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Longhorn volume {{ $labels.volume }} is unhealthy"
          description: "Volume is not in healthy/robust state."

      - alert: LonghornNodeDown
        expr: longhorn_node_status{condition="ready"} != 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Longhorn node {{ $labels.node }} is not ready"
          description: "Storage node is not in ready state."

      - alert: LonghornDiskSpaceLow
        expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Longhorn disk space low on {{ $labels.node }}"
          description: "Longhorn storage disk usage is above 80%."

    # ============================================
    # MONITORING STACK ALERTS
    # ============================================
    - name: monitoring-alerts
      rules:
      - alert: PrometheusDown
        expr: kube_deployment_status_replicas_available{namespace="monitoring",deployment="prometheus-k8s"} < 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Prometheus is down"
          description: "Prometheus server is not running."

      - alert: PrometheusTargetDown
        expr: up == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus target {{ $labels.job }}/{{ $labels.instance }} is down"
          description: "Scrape target has been unreachable for more than 5 minutes."

      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus config reload failed"
          description: "Prometheus failed to reload its configuration."

      - alert: GrafanaDown
        expr: absent(kube_deployment_status_replicas_available{namespace="monitoring",deployment="grafana"} >= 1)
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Grafana is down"
          description: "No healthy Grafana pods detected."

    # ============================================
    # APPLICATION ALERTS
    # ============================================
    - name: application-alerts
      rules:
      # Main app (tr)
      - alert: TRAppDown
        expr: kube_deployment_status_replicas_available{namespace="tr",deployment="tr-deployment"} < 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "TR application is down"
          description: "No healthy tr-deployment pods available."

      - alert: TRAppDegraded
        expr: kube_deployment_status_replicas_available{namespace="tr",deployment="tr-deployment"} < kube_deployment_spec_replicas{namespace="tr",deployment="tr-deployment"}
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "TR application is degraded"
          description: "Not all replicas are available ({{ $value }} of expected)."

      # Tauri app
      - alert: TauriAppDown
        expr: kube_deployment_status_replicas_available{namespace="tauri",deployment=~"tauri.*"} < 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Tauri application is down"
          description: "No healthy Tauri pods available."

      # Database health (PostgreSQL via kube-state-metrics)
      - alert: PostgresDown
        expr: kube_statefulset_status_replicas_ready{statefulset=~".*postgres.*"} < 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL {{ $labels.namespace }}/{{ $labels.statefulset }} is down"
          description: "No healthy PostgreSQL pods detected."

    # ============================================
    # API SERVER ALERTS
    # ============================================
    - name: apiserver-alerts
      rules:
      - alert: KubernetesAPIServerDown
        expr: up{job="kubernetes-apiservers"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes API server is down"
          description: "API server {{ $labels.instance }} is not responding."

      - alert: KubernetesAPIServerHighLatency
        expr: histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{verb!="WATCH"}[5m])) by (le, verb)) > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High API server latency"
          description: "99th percentile latency for {{ $labels.verb }} requests is above 1 second."

      - alert: KubernetesAPIServerErrorsHigh
        expr: sum(rate(apiserver_request_total{code=~"5.."}[5m])) / sum(rate(apiserver_request_total[5m])) * 100 > 3
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High API server error rate"
          description: "More than 3% of API requests are failing."

      - alert: EtcdHighCommitDuration
        expr: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) > 0.25
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High etcd commit duration"
          description: "99th percentile etcd commit duration is above 250ms."
